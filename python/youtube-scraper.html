<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
   
    <title>YouTube Scraper - Python - David Kohreidze</title> 
    <meta name="description" content="Personal site of David Kohreidze.">
    <meta name="robots" content="noindex">

    <link rel="shortcut icon" type="image/png" href="../img/favicon.ico"/>

    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="../font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- Prism Syntax Highlighter -->
    <link href="../css/prism.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="http://dkohreidze.com/">
                    <span class="light">David</span> Kohreidze
                </a>
            </div>

            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden <li> to remove active class from about link when scrolled up past about section -->
                    <li class="hidden"><a href=""></a></li>
                    <li><a class="page-scroll" href="http://dkohreidze.com/#about">About</a></li>
                    <li><a class="page-scroll" href="http://dkohreidze.com/#skills">Skills</a></li>
                    <li><a class="page-scroll" href="http://dkohreidze.com/#resume">Resume</a></li>
                    <li><a class="page-scroll" href="http://dkohreidze.com/#contact">Contact</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">YouTube Scraper</h1>
                        <p class="intro-text">Written in Python using <a href="http://docs.python-requests.org/en/latest/" target="_blank">Requests</a> & <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank">Beautiful Soup</a></p>
                        <a href="#article" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Article Section -->
    <section id="article" class="container article-content content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <div class="article-data">
                    <h5>October 12, 2015</h5>
                </div>                
                <p>
                    This is a lightweight scraper I made to harvest email addresses for YouTube influencers and channels. It's written in Python using <a href="http://docs.python-requests.org/en/latest/" target="_blank">Requests</a>, an HTTP library, and <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank">Beautiful Soup</a>, a parsing library.
                </p>
                <p>
                    Want to use the script or adapt it for another website? Fork it on GitHub.
                </p>  
                <pre>
                    <code class="language-python">
#!/usr/bin/python
# David Kohreidze
# youtube-scraper.py

# Library documentation
# http://docs.python-requests.org/en/latest/user/quickstart/
# http://www.crummy.com/software/BeautifulSoup/bs4/doc/

import csv
import re
import requests
import time
from bs4 import BeautifulSoup

# scrapes the title 
def get_title():
    d = soup.find_all("h1", "branded-page-header-title")
    for i in d:
        title = i.text.strip().replace('\n',' ').replace(',','').encode("utf-8") 
        f.write(title+',')
        print('\t%s') % (title)

# scrapes the subscriber count
def get_subs():
    b = soup.find_all("span", "about-stat")
    for i in b:
        try:            
            value = i.b.text.strip().replace(',','')                    
            if len(b) == 3:
                f.write(value+',')
                print('\t%s') %(value)
            elif len(b) == 2:
                f.write('null,'+ value + ',')
                print('\tsubs = null\n\t%s') %(value)
            else:
                f.write('null,null,')
                print('\tsubs = null\nviews = null')
        except AttributeError:
            pass

# scrapes the description
def get_description():
    c = soup.find_all("div", "about-description")
    if c:
        for i in c:
            description = i.text.strip().replace('\n',' ').replace(',','').encode("utf-8")      
            f.write(description+',')
            print('\t%s') % (description)
            

            regex = re.compile(("([a-z0-9!#$%&'*+\/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+\/=?^_`"
                                "{|}~-]+)*(@|\sat\s|\[at\])(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?(\.|"
                                "\sdot\s))+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)"))
            
            email = re.search(regex, description)
            if email:
                if not email.group(0).startswith('//'):
                    print('\tEmail = ' + email.group())
                    f.write(email.group(0)+',')
            else:
                print('\tEmail = null')
                f.write('null,')
    else:
        print('\tDescription = null\n\tEmail = null')
        f.write('null,null,')

        

# scrapes all the external links 
def get_links():
    a = soup.find_all("a", "about-channel-link ") # trailing space is required.
    for i in a:
        url = i.get('href')
        f.write(url+',')
        print('\t%s') % (url)

# scrapes the related channels
def get_related():
    s = soup.find_all("h3", "yt-lockup-title")
    for i in s:
        t = i.find_all(href=re.compile("user"))
        for i in t:
            url = 'https://www.youtube.com'+i.get('href')
            related.write(url+'\n')
            #print('\t\t%s,%s') % (i.text, url) 

# create output files
f = open("youtube-scrape-data.csv", "w+") 
related = open("related-channels.csv", "w+")

# empy list to save pages we've already scraped
visited = []

# disassemble YouTube search result page URL
base = "https://www.youtube.com/results?search_query="
page = "&page="
q = ['dog+training', 'dog+behavior', 'dog+health'] # enumerate all keywords here
count = 1 # start on page 1
pagesToScrape = 1 # the number of search result pages to scrape
timeToSleep = 3 # the number of seconds between pings to the YouTube server

# set outout csv file column labels
f.write('url, title, subs, views, description, email, external links\n')

for query in q:
    while count <= pagesToScrape:
        # assemble the URL to scrape
        scrapeURL = base + str(query) + page + str(count)
        
        print('\nScraping %s\n') %(scrapeURL)
        
        # ping and retrieve search result page HTML 
        r = requests.get(scrapeURL)

        # create Soup object from HTML 
        soup = BeautifulSoup(r.text)

        # parse channel container
        users = soup.find_all("div", "yt-lockup-byline")
        
        for each in users:
            # parse all URLs that contain 'user'
            a = each.find_all(href=re.compile("user"))
            for i in a:
                # assemble channel's about page; this is where our data is located
                url = 'https://www.youtube.com'+i.get('href')+'/about'
                
                # check to see if channel has already been scraped
                if url in visited:
                    print('\t%s has already been scraped\n\n') %(url)
                else:
                    # ping and retreive channel's HTML, store as Soup object
                    r = requests.get(url)
                    soup = BeautifulSoup(r.text)

                    # output channel url to csv file & terminal
                    f.write(url + ',')
                    print('\t%s') %(url)

                    # scrape the meta data
                    get_title()
                    get_subs()
                    get_description()
                    get_links()
                    get_related()

                    # formatting csv & terminal output
                    f.write('\n')   
                    print('\n')

                    # add url to visited list
                    visited.append(url)

                    # time delay between pings to YouTube server
                    time.sleep(timeToSleep)
        
        # iterate to the next search result page
        count += 1
        print('\n')
    
    # reset page count as we've iterated to next search term
    count = 1
    print('\n') 
f.close()
                    </code>
                </pre>
            </div>
        </div>
    </section>

    <!-- Resume Section -->
    <section id="resume" class="content-section text-center">
        <div class="download-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Resume</h2>
                    <p>Updated on 7 October 2015.</p>
                    <a href="http://dkohreidze.com/resume.pdf" target="_blank" class="btn btn-default btn-lg" onClick="ga('send', 'event', 'Button', 'Click', 'Resume');">View Resume</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>&copy; 2015</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="../js/jquery.js"></script>
    
    <!-- Bootstrap Core JavaScript -->
    <script src="../js/bootstrap.min.js"></script>

    <!-- JavaScript Easing Plugin -->
    <script src="../js/jquery.easing.min.js"></script>

    <!-- Custom JavaScript -->
    <script src="../js/grayscale.js"></script>

    <!-- Prism Syntax Highlighter -->
    <script src="../js/prism.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-68548205-1', 'auto');
      ga('send', 'pageview');
    </script>

</body>
</html>